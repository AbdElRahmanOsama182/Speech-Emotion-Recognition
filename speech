{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1877714,"sourceType":"datasetVersion","datasetId":1118008}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pattern Recognition Assignment 2: Speech Emotion Recognition\n**Team Members:**\n- AbdElRahman Bassam - 21010729\n- AbdElRahman Osama - 21010717\n- Ahmed Youssef - 21010217","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nimport os\nimport math\nimport IPython.display as ipd\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nimport glob\nimport itertools\nimport json\n\nRANDOM_SEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:52:40.768159Z","iopub.execute_input":"2025-05-15T06:52:40.768416Z","iopub.status.idle":"2025-05-15T06:52:47.996426Z","shell.execute_reply.started":"2025-05-15T06:52:40.768394Z","shell.execute_reply":"2025-05-15T06:52:47.995647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Exploration","metadata":{}},{"cell_type":"code","source":"CREMA_PATH = '../input/speech-emotion-recognition-en/Crema/'\n\nEMOTION_MAPPING = {\n    \"SAD\" : \"sadness\",\n    \"ANG\" : \"angry\",\n    \"DIS\" : \"disgust\",\n    \"FEA\" : \"fear\",\n    \"HAP\" : \"happy\",\n    \"NEU\" : \"neutral\"\n}\n\nfemale_ids = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,\n              1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,1052,1053,1054,\n              1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,\n              1082,1084,1089,1091]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:52:47.997550Z","iopub.execute_input":"2025-05-15T06:52:47.997975Z","iopub.status.idle":"2025-05-15T06:52:48.002490Z","shell.execute_reply.started":"2025-05-15T06:52:47.997949Z","shell.execute_reply":"2025-05-15T06:52:48.001806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load audio from path, with sr as default=16KHz (to resample the audio to unique sr)\ndef load_audio(path, sr=16000):\n  audio, sr = librosa.load(path, sr=sr)\n  return audio, sr\n\ndef get_emotion_from_filename(filename):\n  return EMOTION_MAPPING.get(filename.split(\"_\")[2])\n\ndef get_gender_from_filename(filename):\n    actor_id = int(filename.split(\"_\")[0])  # Assuming the actor ID is the first part of the filename\n    return 'female' if actor_id in female_ids else 'male'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:52:48.003232Z","iopub.execute_input":"2025-05-15T06:52:48.003480Z","iopub.status.idle":"2025-05-15T06:52:48.015436Z","shell.execute_reply.started":"2025-05-15T06:52:48.003464Z","shell.execute_reply":"2025-05-15T06:52:48.014910Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def explore_audio(audio_path):\n    filename = os.path.basename(audio_path)\n    print(f\"\\nExploring: {filename}\")\n    audio, sr = load_audio(path=audio_path, sr=None)\n    ipd.display(ipd.Audio(audio, rate=sr))\n    plt.figure(figsize=(10, 3))\n    librosa.display.waveshow(audio, sr=sr)\n    plt.suptitle(f\"Waveform - {filename}\")\n    plt.title(f\"Emotion: {get_emotion_from_filename(filename)}\")\n    plt.xlabel(\"Time (s)\")\n    plt.ylabel(\"Amplitude\")\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\ndef explore_dataset(dataset_path):\n  emotions_visited = set()\n  one_sample_per_emotion = {}\n\n  for filename in os.listdir(dataset_path):\n    if filename.endswith(\".wav\"):\n      emotion = get_emotion_from_filename(filename)\n      if emotion in emotions_visited:\n        continue\n      one_sample_per_emotion[emotion] = os.path.join(dataset_path, filename)\n      emotions_visited.add(emotion)\n      if len(emotions_visited) == len(EMOTION_MAPPING):\n        break\n\n  for emotion, path in one_sample_per_emotion.items():\n    print(f\"\\nEmotion: {emotion}\")\n    explore_audio(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:52:48.017086Z","iopub.execute_input":"2025-05-15T06:52:48.017330Z","iopub.status.idle":"2025-05-15T06:52:48.027345Z","shell.execute_reply.started":"2025-05-15T06:52:48.017314Z","shell.execute_reply":"2025-05-15T06:52:48.026824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"explore_dataset(CREMA_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:52:48.028052Z","iopub.execute_input":"2025-05-15T06:52:48.028264Z","iopub.status.idle":"2025-05-15T06:53:01.931697Z","shell.execute_reply.started":"2025-05-15T06:52:48.028241Z","shell.execute_reply":"2025-05-15T06:53:01.930957Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating Feature Spaces","metadata":{}},{"cell_type":"code","source":"# Extract zero-crossing-rate sequence\ndef extract_zcr_seq(y, frame_length=2048, hop_length=512):\n    zcr_seq = librosa.feature.zero_crossing_rate(y, frame_length=frame_length, hop_length=hop_length)[0]\n    return zcr_seq\n\n# Extract Root Mean Square sequence\ndef extract_rms_seq(y, frame_length=2048, hop_length=512):\n    rms_seq = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n    return rms_seq\n\n# Extract Mel Spectrogram with 128 Mel bands\ndef extract_mel_spectrogram(y, sr, n_mels=128, frame_length=2048, hop_length=512):\n    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=frame_length, hop_length=hop_length, n_mels=n_mels)\n    mel_spec_db = librosa.power_to_db(mel_spec)\n    return mel_spec_db","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:53:01.932399Z","iopub.execute_input":"2025-05-15T06:53:01.932802Z","iopub.status.idle":"2025-05-15T06:53:01.941679Z","shell.execute_reply.started":"2025-05-15T06:53:01.932771Z","shell.execute_reply":"2025-05-15T06:53:01.940867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_zcr = [] # zero-crossing-rate\n# X_rms = [] # RMS\n# X_mel = [] # mel spectogram data\ny = [] # labels\ny_gender = [] # labels consedring the gender\nmx_audio_len = 0\n# find max audio len\nfor filename in os.listdir(CREMA_PATH):\n  if filename.endswith(\".wav\"):\n    filepath = os.path.join(CREMA_PATH, filename)\n    audio, _ = load_audio(filepath)\n    mx_audio_len = max(mx_audio_len,len(audio))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:53:01.942753Z","iopub.execute_input":"2025-05-15T06:53:01.943086Z","iopub.status.idle":"2025-05-15T06:54:10.424673Z","shell.execute_reply.started":"2025-05-15T06:53:01.943056Z","shell.execute_reply":"2025-05-15T06:54:10.423956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for filename in sorted(os.listdir(CREMA_PATH)):\n#   if filename.endswith(\".wav\"):\n#     filepath = os.path.join(CREMA_PATH, filename)\n#     audio, sr = load_audio(filepath)\n#     # pad center the audio so all have same length\n#     audio = librosa.util.pad_center(audio, size=mx_audio_len)\n#     X_zcr.append(extract_zcr_seq(audio))\n#     X_rms.append(extract_rms_seq(audio))\n#     X_mel.append(extract_mel_spectrogram(audio, sr))\n#     emotion = get_emotion_from_filename(filepath)\n#     gender = get_gender_from_filename(filename)\n#     y.append(emotion)\n#     y_gender.append(f\"{emotion}_{gender}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:54:10.425927Z","iopub.execute_input":"2025-05-15T06:54:10.426197Z","iopub.status.idle":"2025-05-15T06:54:10.430417Z","shell.execute_reply.started":"2025-05-15T06:54:10.426174Z","shell.execute_reply":"2025-05-15T06:54:10.429751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CACHE_DIR = \"/kaggle/working/saved_features\"\nos.makedirs(CACHE_DIR, exist_ok=True)\n\nX_zcr, X_rms, X_mel, y, y_gender = [], [], [], [], []\n\nfor filename in sorted(os.listdir(CREMA_PATH)):\n    if filename.endswith(\".wav\"):\n        cache_path = os.path.join(CACHE_DIR, f\"{filename}.npz\")\n\n        if os.path.exists(cache_path):\n            data = np.load(cache_path)\n            zcr, rms, mel = data['zcr'], data['rms'], data['mel']\n        else:\n            filepath = os.path.join(CREMA_PATH, filename)\n            audio, sr = load_audio(filepath)\n            audio = librosa.util.pad_center(audio, size=mx_audio_len)\n            zcr = extract_zcr_seq(audio)\n            rms = extract_rms_seq(audio)\n            mel = extract_mel_spectrogram(audio, sr)\n            np.savez(cache_path, zcr=zcr, rms=rms, mel=mel)\n\n        X_zcr.append(zcr)\n        X_rms.append(rms)\n        X_mel.append(mel)\n\n        emotion = get_emotion_from_filename(filename)\n        gender = get_gender_from_filename(filename)\n        y.append(emotion)\n        y_gender.append(f\"{emotion}_{gender}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:54:10.431303Z","iopub.execute_input":"2025-05-15T06:54:10.432080Z","iopub.status.idle":"2025-05-15T06:58:05.349019Z","shell.execute_reply.started":"2025-05-15T06:54:10.432046Z","shell.execute_reply":"2025-05-15T06:58:05.348252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_zcr = np.array(X_zcr)\nX_energy = np.array(X_rms)\nX_mel = np.array(X_mel)\ny = np.array(y)\ny_gender = np.array(y_gender)\nprint(np.shape(X_zcr))\nprint(np.shape(X_rms))\nprint(np.shape(X_mel))\nprint(np.shape(y))\nprint(np.shape(y_gender))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:05.351879Z","iopub.execute_input":"2025-05-15T06:58:05.352326Z","iopub.status.idle":"2025-05-15T06:58:05.563421Z","shell.execute_reply.started":"2025-05-15T06:58:05.352299Z","shell.execute_reply":"2025-05-15T06:58:05.562695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualize 3 random samples in feature spaces\nnp.random.seed(RANDOM_SEED)\nindices = np.random.choice(X_mel.shape[0], size=3, replace=False)\n\nplt.figure(figsize=(20, 10))\n\n# Plot mel spectograms\nfor i, idx in enumerate(indices):\n    plt.subplot(3, 3, i+1)\n    librosa.display.specshow(X_mel[idx], sr=16000, x_axis='time', y_axis='mel')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(f\"Sample #{idx}\")\n\n# Plot RMSE\nfor i, idx in enumerate(indices):\n    plt.subplot(3, 3, i+4)\n    plt.plot(X_rms[idx])\n    plt.title(f\"RMSE - Sample #{idx}\")\n    plt.xlabel(\"Frame\")\n    plt.ylabel(\"RMSE\")\n\n# Plot ZCR\nfor i, idx in enumerate(indices):\n    plt.subplot(3, 3, i+7)\n    plt.plot(X_zcr[idx])\n    plt.title(f\"ZCR - Sample #{idx}\")\n    plt.xlabel(\"Frame\")\n    plt.ylabel(\"ZCR\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:05.564247Z","iopub.execute_input":"2025-05-15T06:58:05.564492Z","iopub.status.idle":"2025-05-15T06:58:07.146397Z","shell.execute_reply.started":"2025-05-15T06:58:05.564473Z","shell.execute_reply":"2025-05-15T06:58:07.145628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting class frequencies\nplt.figure(figsize=(10, 6))\n\nclass_counts = np.unique(y, return_counts=True)\nsns.barplot(x=class_counts[0], y=class_counts[1])\n\nplt.title('Class Frequency Distribution')\nplt.xlabel('Emotion Classes')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:07.147273Z","iopub.execute_input":"2025-05-15T06:58:07.147492Z","iopub.status.idle":"2025-05-15T06:58:07.525706Z","shell.execute_reply.started":"2025-05-15T06:58:07.147476Z","shell.execute_reply":"2025-05-15T06:58:07.524932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting class frequencies\nplt.figure(figsize=(10, 6))\n\nclass_counts = np.unique(y_gender, return_counts=True)\nsns.barplot(x=class_counts[0], y=class_counts[1])\n\nplt.title('Class Frequency Distribution Consedring Gender)')\nplt.xlabel('Emotion_Gender Classes')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:07.526495Z","iopub.execute_input":"2025-05-15T06:58:07.526797Z","iopub.status.idle":"2025-05-15T06:58:07.727236Z","shell.execute_reply.started":"2025-05-15T06:58:07.526773Z","shell.execute_reply":"2025-05-15T06:58:07.726420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_time = np.stack([X_zcr, X_rms], axis=1) # stack time features in to one feature space\nprint(X_time.shape)\n# Add channel dimension at axis=1 -> shape becomes (7442, 1, 128, 157)\nX_mel = X_mel[:, np.newaxis, :, :]\nprint(X_mel.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:07.727898Z","iopub.execute_input":"2025-05-15T06:58:07.728112Z","iopub.status.idle":"2025-05-15T06:58:07.744018Z","shell.execute_reply.started":"2025-05-15T06:58:07.728097Z","shell.execute_reply":"2025-05-15T06:58:07.743363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_data(X_time, X_mel, y, y_gender, test_size=0.3, val_size=0.05, random_state=42):\n    # First split into train+val and test\n    idx_trainval, idx_test = train_test_split(\n        np.arange(len(y)), stratify=y, test_size=test_size, random_state=random_state\n    )\n\n    # Then split train+val into train and val\n    y_trainval = y[idx_trainval]\n    idx_train, idx_val = train_test_split(\n        idx_trainval, stratify=y_trainval, test_size=val_size, random_state=random_state\n    )\n\n    # Use indices to slice all arrays\n    def split_arrays(arr):\n        return arr[idx_train], arr[idx_val], arr[idx_test]\n\n    X_time_train, X_time_val, X_time_test = split_arrays(X_time)\n    X_mel_train, X_mel_val, X_mel_test = split_arrays(X_mel)\n    y_train, y_val, y_test = split_arrays(y)\n    y_gender_train, y_gender_val, y_gender_test = split_arrays(y_gender)\n\n    return (X_time_train, X_time_val, X_time_test,\n            X_mel_train, X_mel_val, X_mel_test,\n            y_train, y_val, y_test,\n            y_gender_train, y_gender_val, y_gender_test,\n            idx_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:07.744844Z","iopub.execute_input":"2025-05-15T06:58:07.745049Z","iopub.status.idle":"2025-05-15T06:58:07.751163Z","shell.execute_reply.started":"2025-05-15T06:58:07.745033Z","shell.execute_reply":"2025-05-15T06:58:07.750431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(X_time_train, X_time_val, X_time_test,\n X_mel_train, X_mel_val, X_mel_test,\n y_train, y_val, y_test,\n y_train_g, y_val_g, y_test_g, idx_train) = split_data(X_time, X_mel, y, y_gender)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:07.751919Z","iopub.execute_input":"2025-05-15T06:58:07.752210Z","iopub.status.idle":"2025-05-15T06:58:07.952515Z","shell.execute_reply.started":"2025-05-15T06:58:07.752186Z","shell.execute_reply":"2025-05-15T06:58:07.951937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"code","source":"train_set_names = np.array(sorted(glob.glob(os.path.join(CREMA_PATH, \"*.wav\"))))[idx_train]\nsample_audio_path = train_set_names[0]\nsample_audio, sr = load_audio(sample_audio_path,16000)\nsample_audio = librosa.util.pad_center(sample_audio, size=mx_audio_len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:07.953119Z","iopub.execute_input":"2025-05-15T06:58:07.953318Z","iopub.status.idle":"2025-05-15T06:58:07.975883Z","shell.execute_reply.started":"2025-05-15T06:58:07.953302Z","shell.execute_reply":"2025-05-15T06:58:07.975372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sr, n_steps=-1):\n    return librosa.effects.pitch_shift(y=data, sr=sr, n_steps=n_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:07.976536Z","iopub.execute_input":"2025-05-15T06:58:07.976806Z","iopub.status.idle":"2025-05-15T06:58:07.981277Z","shell.execute_reply.started":"2025-05-15T06:58:07.976779Z","shell.execute_reply":"2025-05-15T06:58:07.980706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Original Audio\")\nipd.display(ipd.Audio(sample_audio, rate=sr))\nplt.figure(figsize=(14, 4))\nlibrosa.display.waveshow(sample_audio, sr=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:07.981952Z","iopub.execute_input":"2025-05-15T06:58:07.982252Z","iopub.status.idle":"2025-05-15T06:58:08.308023Z","shell.execute_reply.started":"2025-05-15T06:58:07.982230Z","shell.execute_reply":"2025-05-15T06:58:08.307296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"With Noise Audio\")\naug = noise(sample_audio)\nipd.display(ipd.Audio(aug, rate=sr))\nplt.figure(figsize=(14, 4))\nlibrosa.display.waveshow(aug, sr=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:08.308943Z","iopub.execute_input":"2025-05-15T06:58:08.309227Z","iopub.status.idle":"2025-05-15T06:58:08.694385Z","shell.execute_reply.started":"2025-05-15T06:58:08.309204Z","shell.execute_reply":"2025-05-15T06:58:08.693643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"With Lower Pitch Audio\")\naug = pitch(sample_audio,sr)\nipd.display(ipd.Audio(aug, rate=sr))\nplt.figure(figsize=(14, 4))\nlibrosa.display.waveshow(aug, sr=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:08.695013Z","iopub.execute_input":"2025-05-15T06:58:08.695203Z","iopub.status.idle":"2025-05-15T06:58:09.838949Z","shell.execute_reply.started":"2025-05-15T06:58:08.695189Z","shell.execute_reply":"2025-05-15T06:58:09.838225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Shifted Audio\")\naug = shift(sample_audio)\nipd.display(ipd.Audio(aug, rate=sr))\nplt.figure(figsize=(14, 4))\nlibrosa.display.waveshow(aug, sr=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:09.839791Z","iopub.execute_input":"2025-05-15T06:58:09.839994Z","iopub.status.idle":"2025-05-15T06:58:10.157674Z","shell.execute_reply.started":"2025-05-15T06:58:09.839978Z","shell.execute_reply":"2025-05-15T06:58:10.156873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def augment(audio,sr):\n    return noise(audio), pitch(audio,sr), shift(audio)\n\nis_augmented = False\nif is_augmented:\n    aug_X_time = []\n    aug_X_mel = []\n    aug_y = []\n    aug_y_gender = []\n    for path in train_set_names:\n        filename = os.path.basename(path)\n        audio,sr = load_audio(path)\n        audio = librosa.util.pad_center(sample_audio, size=mx_audio_len)\n        np.random.seed(RANDOM_SEED)\n        audio_noise, audio_pitch, audio_shift = augment(audio, sr)\n        emotion = get_emotion_from_filename(filename)\n        gender = get_gender_from_filename(filename)\n    \n        # Extract features for each augmentation\n        for aug_audio in [audio_noise, audio_pitch, audio_shift]:\n            zcr = extract_zcr_seq(aug_audio)\n            rms = extract_rms_seq(aug_audio)\n            mel = extract_mel_spectrogram(aug_audio, sr)\n    \n            time_feats = np.stack([zcr, rms], axis=0) # Stack time features -> (2, 157)\n            mel = mel[np.newaxis, :, :]  # Add channel dim -> (1, 128, 157)\n    \n            # Append to lists\n            aug_X_time.append(time_feats)\n            aug_X_mel.append(mel)\n            aug_y.append(emotion)\n            aug_y_gender.append(gender)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.158628Z","iopub.execute_input":"2025-05-15T06:58:10.159257Z","iopub.status.idle":"2025-05-15T06:58:10.164926Z","shell.execute_reply.started":"2025-05-15T06:58:10.159237Z","shell.execute_reply":"2025-05-15T06:58:10.164260Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(X_time_train)) # get number of samples in training before augmentation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.165743Z","iopub.execute_input":"2025-05-15T06:58:10.165964Z","iopub.status.idle":"2025-05-15T06:58:10.177045Z","shell.execute_reply.started":"2025-05-15T06:58:10.165942Z","shell.execute_reply":"2025-05-15T06:58:10.176458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if is_augmented:\n    X_time_train = np.concatenate([X_time_train, aug_X_time], axis=0)\n    X_mel_train = np.concatenate([X_mel_train, aug_X_mel], axis=0)\n    y_train = np.concatenate([y_train, aug_y], axis=0)\n    y_train_g = np.concatenate([y_train_g, aug_y_gender], axis=0)\n    \n    print(X_time_train.shape)\n    print(X_mel_train.shape)\n    print(y_train.shape)\n    print(y_train_g.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.177681Z","iopub.execute_input":"2025-05-15T06:58:10.177838Z","iopub.status.idle":"2025-05-15T06:58:10.190066Z","shell.execute_reply.started":"2025-05-15T06:58:10.177825Z","shell.execute_reply":"2025-05-15T06:58:10.189398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if is_augmented:\n    # Shuffle the training set\n    num_samples = X_time_train.shape[0]\n    \n    np.random.seed(RANDOM_SEED)\n    # Generate a shuffled index\n    shuffled_indices = np.random.permutation(num_samples)\n    \n    # Shuffle all arrays using the same indices\n    X_time_train = X_time_train[shuffled_indices]\n    X_mel_train = X_mel_train[shuffled_indices]\n    y_train = y_train[shuffled_indices]\n    y_train_g = y_train_g[shuffled_indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.190694Z","iopub.execute_input":"2025-05-15T06:58:10.190901Z","iopub.status.idle":"2025-05-15T06:58:10.202007Z","shell.execute_reply.started":"2025-05-15T06:58:10.190880Z","shell.execute_reply":"2025-05-15T06:58:10.201322Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Pipeline","metadata":{}},{"cell_type":"code","source":"normalize = False\nif normalize:\n    # Initialize scalers for ZCR and RMS\n    scaler_zcr = StandardScaler()\n    scaler_rms = StandardScaler()\n    \n    # Fit on training data (X_time_train) and transform all datasets\n    X_time_train[:, 0, :] = scaler_zcr.fit_transform(X_time_train[:, 0, :])  # Normalize ZCR\n    X_time_train[:, 1, :] = scaler_rms.fit_transform(X_time_train[:, 1, :])  # Normalize RMS\n    \n    # Apply the same transformation to validation and test sets\n    X_time_val[:, 0, :] = scaler_zcr.transform(X_time_val[:, 0, :])\n    X_time_val[:, 1, :] = scaler_rms.transform(X_time_val[:, 1, :])\n    \n    X_time_test[:, 0, :] = scaler_zcr.transform(X_time_test[:, 0, :])\n    X_time_test[:, 1, :] = scaler_rms.transform(X_time_test[:, 1, :])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.202774Z","iopub.execute_input":"2025-05-15T06:58:10.202955Z","iopub.status.idle":"2025-05-15T06:58:10.212311Z","shell.execute_reply.started":"2025-05-15T06:58:10.202941Z","shell.execute_reply":"2025-05-15T06:58:10.211629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# One-hot encode\nohe = OneHotEncoder(sparse_output=False)\ny_train_oh = ohe.fit_transform(y_train.reshape(-1, 1))\ny_val_oh = ohe.transform(y_val.reshape(-1, 1))\ny_test_oh = ohe.transform(y_test.reshape(-1, 1))\n\nohe_g = OneHotEncoder(sparse_output=False)\ny_train_oh_g = ohe_g.fit_transform(y_train_g.reshape(-1, 1))\ny_val_oh_g = ohe_g.transform(y_val_g.reshape(-1, 1))\ny_test_oh_g = ohe_g.transform(y_test_g.reshape(-1, 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.216562Z","iopub.execute_input":"2025-05-15T06:58:10.216795Z","iopub.status.idle":"2025-05-15T06:58:10.239855Z","shell.execute_reply.started":"2025-05-15T06:58:10.216780Z","shell.execute_reply":"2025-05-15T06:58:10.239063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CremaDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n# Convert data to the correct dtype\nX_time_train = X_time_train.astype(np.float32)\nX_time_val = X_time_val.astype(np.float32)\nX_time_test = X_time_test.astype(np.float32)\n\ny_train_oh = y_train_oh.astype(np.float32)\ny_val_oh = y_val_oh.astype(np.float32)\ny_test_oh = y_test_oh.astype(np.float32)\ny_train_oh_g = y_train_oh_g.astype(np.float32)\ny_val_oh_g = y_val_oh_g.astype(np.float32)\ny_test_oh_g = y_test_oh_g.astype(np.float32)\n\nbatch_size = 64\n\n# Create datasets for both y and y_gender\ntime_train_loader = DataLoader(CremaDataset(X_time_train, y_train_oh), batch_size=batch_size, shuffle=True)\ntime_val_loader = DataLoader(CremaDataset(X_time_val, y_val_oh), batch_size=batch_size)\ntime_test_loader = DataLoader(CremaDataset(X_time_test, y_test_oh), batch_size=batch_size)\n\ntime_train_loader_g = DataLoader(CremaDataset(X_time_train, y_train_oh_g), batch_size=batch_size, shuffle=True)\ntime_val_loader_g = DataLoader(CremaDataset(X_time_val, y_val_oh_g), batch_size=batch_size)\ntime_test_loader_g = DataLoader(CremaDataset(X_time_test, y_test_oh_g), batch_size=batch_size)\n\n# Repeat for mel data\nmel_train_loader = DataLoader(CremaDataset(X_mel_train, y_train_oh), batch_size=batch_size, shuffle=True)\nmel_val_loader = DataLoader(CremaDataset(X_mel_val, y_val_oh), batch_size=batch_size)\nmel_test_loader = DataLoader(CremaDataset(X_mel_test, y_test_oh), batch_size=batch_size)\n\nmel_train_loader_g = DataLoader(CremaDataset(X_mel_train, y_train_oh_g), batch_size=batch_size, shuffle=True)\nmel_val_loader_g = DataLoader(CremaDataset(X_mel_val, y_val_oh_g), batch_size=batch_size)\nmel_test_loader_g = DataLoader(CremaDataset(X_mel_test, y_test_oh_g), batch_size=batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.240768Z","iopub.execute_input":"2025-05-15T06:58:10.241083Z","iopub.status.idle":"2025-05-15T06:58:10.251682Z","shell.execute_reply.started":"2025-05-15T06:58:10.241046Z","shell.execute_reply":"2025-05-15T06:58:10.251141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TimeNet(nn.Module):\n    def __init__(self, num_classes):\n        super(TimeNet, self).__init__()\n\n        self.relu = nn.ReLU()\n\n        self.conv1 = nn.Conv1d(in_channels=2, out_channels=256, kernel_size=3, stride=2, padding=1)  # (B, 256, 78)\n        self.pool1 = nn.MaxPool1d(kernel_size=2)  # (B, 256, 39)\n\n        self.conv2 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)  # (B, 128, 39)\n        self.pool2 = nn.MaxPool1d(kernel_size=2)  # (B, 128, 19)\n\n        self.conv3 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, padding=1)  # (B, 128, 19)\n        self.pool3 = nn.MaxPool1d(kernel_size=2)  # (B, 128, 9)\n\n        self.dropout1 = nn.Dropout(0.2)\n\n        self.conv4 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1)  # (B, 64, 9)\n        self.pool4 = nn.AvgPool1d(kernel_size=2)  # (B, 64, 4)\n\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(64 * 4, 32)\n        self.dropout2 = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(32, num_classes)\n\n    def forward(self, x):  # x: (B, 2, 157)\n        x = self.relu(self.conv1(x))   # (B, 256, 78)\n        x = self.pool1(x)              # (B, 256, 39)\n\n        x = self.relu(self.conv2(x))   # (B, 128, 39)\n        x = self.pool2(x)              # (B, 128, 19)\n\n        x = self.relu(self.conv3(x))   # (B, 128, 19)\n        x = self.pool3(x)              # (B, 128, 9)\n\n        x = self.dropout1(x)\n\n        x = self.relu(self.conv4(x))   # (B, 64, 9)\n        x = self.pool4(x)              # (B, 64, 4)\n\n        x = self.flatten(x)            # (B, 256)\n        x = self.relu(self.fc1(x))     # (B, 32)\n        x = self.dropout2(x)\n        x = self.fc2(x)                # (B, num_classes)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.252671Z","iopub.execute_input":"2025-05-15T06:58:10.252903Z","iopub.status.idle":"2025-05-15T06:58:10.264055Z","shell.execute_reply.started":"2025-05-15T06:58:10.252888Z","shell.execute_reply":"2025-05-15T06:58:10.263450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def confusion_matrix_to_labels(cm):\n    y_true = []\n    y_pred = []\n    for true_label in range(cm.shape[0]):\n        for pred_label in range(cm.shape[1]):\n            count = cm[true_label, pred_label]\n            y_true.extend([true_label] * count)\n            y_pred.extend([pred_label] * count)\n    return np.array(y_true), np.array(y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.264730Z","iopub.execute_input":"2025-05-15T06:58:10.264885Z","iopub.status.idle":"2025-05-15T06:58:10.277782Z","shell.execute_reply.started":"2025-05-15T06:58:10.264873Z","shell.execute_reply":"2025-05-15T06:58:10.277185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_and_pool_confusion_matrix(cm, class_names):\n    # 2x2 sum pooling\n    if cm.shape[0] % 2 != 0 or cm.shape[1] % 2 != 0:\n        raise ValueError(\"Confusion matrix dimensions must be even for 2x2 pooling.\")\n    \n    pooled_cm = []\n    \n    # Perform 2x2 sum pooling\n    for i in range(0, cm.shape[0], 2):\n        row = []\n        for j in range(0, cm.shape[1], 2):\n            block_sum = np.sum(cm[i:i+2, j:j+2])  # Sum the 2x2 block\n            row.append(block_sum)\n        pooled_cm.append(row)\n    \n    pooled_cm = np.array(pooled_cm)\n    y_true, y_pred = confusion_matrix_to_labels(pooled_cm)\n    # Calculate accuracy and F1 score\n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n\n    print(\"Accuracy:\", acc)\n    print(\"F1 Score:\", f1)\n    print(\"Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=class_names))\n\n    # Display the pooled confusion matrix\n    sns.heatmap(pooled_cm, annot=True, fmt='d', xticklabels=class_names[::], yticklabels=class_names[::], cmap='Blues')\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Pooled Confusion Matrix\")\n    plt.show()\n\n    return acc, f1, pooled_cm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.278492Z","iopub.execute_input":"2025-05-15T06:58:10.278756Z","iopub.status.idle":"2025-05-15T06:58:10.291248Z","shell.execute_reply.started":"2025-05-15T06:58:10.278735Z","shell.execute_reply":"2025-05-15T06:58:10.290529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, y_val, class_names,\n                epochs=100, lr=0.001, patience=10, pool = False,isTuning = False):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    best_val_loss = float('inf')\n    patience_counter = 0\n    best_model_state = None\n\n    train_losses = []\n    val_losses = []\n    train_correct= 0\n    train_total=0\n    for epoch in range(epochs):\n        # Training\n        model.train()\n        train_loss = 0\n        for X_batch, y_batch in train_loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            _, predicted = torch.max(outputs, 1)\n            labels = torch.argmax(y_batch, dim=1)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            train_correct += (predicted == labels).sum().item()\n            train_total += labels.size(0)\n            \n        avg_train_loss = train_loss / len(train_loader)\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                X_batch = X_batch.to(device)\n                y_batch = y_batch.to(device)\n                outputs = model(X_batch)\n                loss = criterion(outputs, torch.argmax(y_batch, dim=1))\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(val_loader)\n\n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n        print(f\"Epoch {epoch+1}/{epochs} - Avg. Train Loss: {avg_train_loss:.4f} - Avg. Val Loss: {avg_val_loss:.4f}\")\n\n        # Early Stopping Check\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            best_model_state = model.state_dict()  # Save best model\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n                break\n    # Plot losses over epochs\n    if(not isTuning):\n        plt.plot(train_losses, label='Avg. Train Loss')\n        plt.plot(val_losses, label='Avg. Val Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('Training vs Validation Loss')\n        plt.xticks(range(0, len(train_losses), 10))\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n    # Load the best model before returning\n    if best_model_state:\n        model.load_state_dict(best_model_state)\n    if(not isTuning):\n        print(\"Training complete.\")\n        train_accuracy = train_correct / train_total\n        print(f\"Total Train Accuracy: {train_accuracy:.4f}\")\n        print(\"Evaluating on validation set...\")\n    acc, f1, cm, report = evaluate_model(model, val_loader, y_val, class_names, pool,isTuning)\n    return model, acc, f1, cm, report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.303498Z","iopub.execute_input":"2025-05-15T06:58:10.303754Z","iopub.status.idle":"2025-05-15T06:58:10.317282Z","shell.execute_reply.started":"2025-05-15T06:58:10.303731Z","shell.execute_reply":"2025-05-15T06:58:10.316551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, data_loader, y_true_onehot, class_names, pool=False,isTuning = False ):\n    model.eval()\n    device = next(model.parameters()).device\n\n    all_preds = []\n\n    with torch.no_grad():\n        for X_batch, _ in data_loader:\n            X_batch = X_batch.to(device)\n            outputs = model(X_batch)\n            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n            all_preds.extend(preds)\n\n    y_true = np.argmax(y_true_onehot, axis=1)\n    acc = accuracy_score(y_true, all_preds)\n    f1 = f1_score(y_true, all_preds, average='weighted')\n    cm = confusion_matrix(y_true, all_preds)\n    report = classification_report(y_true, all_preds, target_names=class_names, output_dict=True)\n    if pool:\n        reduced_class_names = [name.split('_')[0] for i, name in enumerate(class_names) if i % 2 == 0]\n        acc, f1, pooled_cm = evaluate_and_pool_confusion_matrix(cm, reduced_class_names)\n        return acc, f1, pooled_cm, report\n    if(not isTuning):\n        print(\"Accuracy:\", acc)\n        print(\"F1 Score:\", f1)\n        print(\"Classification Report:\")\n        print(classification_report(y_true, all_preds, target_names=class_names))\n        sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(\"Confusion Matrix\")\n        plt.show()\n\n    return acc, f1, cm, report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.291942Z","iopub.execute_input":"2025-05-15T06:58:10.292153Z","iopub.status.idle":"2025-05-15T06:58:10.302711Z","shell.execute_reply.started":"2025-05-15T06:58:10.292129Z","shell.execute_reply":"2025-05-15T06:58:10.302053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed=RANDOM_SEED):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.317993Z","iopub.execute_input":"2025-05-15T06:58:10.318186Z","iopub.status.idle":"2025-05-15T06:58:10.330484Z","shell.execute_reply.started":"2025-05-15T06:58:10.318171Z","shell.execute_reply":"2025-05-15T06:58:10.329799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"set_seed(RANDOM_SEED)\ntime_model = TimeNet(num_classes=y_train_oh.shape[1])\ntime_model, time_acc, time_f1, time_cm, time_report = train_model(time_model, time_train_loader, time_val_loader, y_val_oh, class_names=ohe.categories_[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:10.331270Z","iopub.execute_input":"2025-05-15T06:58:10.331502Z","iopub.status.idle":"2025-05-15T06:58:39.274085Z","shell.execute_reply.started":"2025-05-15T06:58:10.331481Z","shell.execute_reply":"2025-05-15T06:58:39.273513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Evaluating the Time-domain model on Test Set\")\ntest_time_acc, test_time_f1, test_time_cm, test_time_report = evaluate_model(time_model, time_test_loader, y_test_oh, class_names=ohe.categories_[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:39.274786Z","iopub.execute_input":"2025-05-15T06:58:39.275240Z","iopub.status.idle":"2025-05-15T06:58:39.570128Z","shell.execute_reply.started":"2025-05-15T06:58:39.275220Z","shell.execute_reply":"2025-05-15T06:58:39.569349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"set_seed(RANDOM_SEED)\ntime_model_g = TimeNet(num_classes=y_train_oh_g.shape[1])\ntime_model_g, time_acc_g, time_f1_g, time_cm_g, time_report_g = train_model(time_model_g, time_train_loader_g, time_val_loader_g, y_val_oh_g, class_names=ohe_g.categories_[0], patience = 30, epochs = 200, pool = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:58:39.570904Z","iopub.execute_input":"2025-05-15T06:58:39.571128Z","iopub.status.idle":"2025-05-15T06:59:46.440541Z","shell.execute_reply.started":"2025-05-15T06:58:39.571110Z","shell.execute_reply":"2025-05-15T06:59:46.439768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Evaluating the Time-domain (Gender) model on Test Set\")\ntest_time_acc_g, test_time_f1_g, test_time_cm_g, test_time_report_g = evaluate_model(time_model_g, time_test_loader_g, y_test_oh_g, class_names=ohe_g.categories_[0], pool = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:59:46.441361Z","iopub.execute_input":"2025-05-15T06:59:46.441864Z","iopub.status.idle":"2025-05-15T06:59:46.730502Z","shell.execute_reply.started":"2025-05-15T06:59:46.441839Z","shell.execute_reply":"2025-05-15T06:59:46.729781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_fused_models(\n    model, report_model, model_g, report_model_g, data_loader, y_true_onehot, class_names\n):\n    model.eval()\n    model_g.eval()\n    device = next(model.parameters()).device\n\n    # Extract F1 scores for coarse model (6 classes)\n    f1_model = np.array([report_model[cls]['f1-score'] for cls in class_names])\n\n    # Pool F1 scores from fine-grained model_g (12 classes)\n    f1_model_g = []\n    for cls in class_names:\n        f1_female = report_model_g.get(f\"{cls}_female\", {'f1-score': 0.0}).get('f1-score', 0.0)\n        f1_male = report_model_g.get(f\"{cls}_male\", {'f1-score': 0.0}).get('f1-score', 0.0)\n        f1_model_g.append(f1_female + f1_male)\n    f1_model_g = np.array(f1_model_g)\n\n    # Normalize the F1 scores to use as weights (total weight = 1)\n    total_f1 = np.sum(f1_model + f1_model_g)\n    f1_model_normalized = (f1_model / total_f1)  # Normalize coarse model F1 scores\n    f1_model_g_normalized = (f1_model_g / total_f1)  # Normalize fine model F1 scores\n\n    # Combine the model weights\n    class_weights = f1_model_normalized + f1_model_g_normalized\n\n    y_true = np.argmax(y_true_onehot, axis=1)\n    all_preds_fused = []\n\n    with torch.no_grad():\n        for X_batch, _ in data_loader:\n            X_batch = X_batch.to(device)\n\n            out_model = torch.softmax(model(X_batch), dim=1).cpu().numpy()         # (B, 6)\n            out_model_g = torch.softmax(model_g(X_batch), dim=1).cpu().numpy()     # (B, 12)\n\n            # Pool the gender-specific outputs\n            out_model_g_pooled = out_model_g[:, ::2] + out_model_g[:, 1::2]  # (B, 6)\n\n            # Fusion with exponential weighting: Prioritize the more confident model\n            alpha = 2.0  # Exponential scaling factor\n            fused_probs = np.exp(alpha * f1_model_normalized) * out_model + np.exp(alpha * f1_model_g_normalized) * out_model_g_pooled\n            fused_probs /= np.sum(np.exp(alpha * np.array([f1_model_normalized, f1_model_g_normalized])), axis=0)\n\n            preds_fused = np.argmax(fused_probs, axis=1)\n            all_preds_fused.extend(preds_fused)\n\n    acc = accuracy_score(y_true, all_preds_fused)\n    f1 = f1_score(y_true, all_preds_fused, average='weighted')\n    report = classification_report(y_true, all_preds_fused, target_names=class_names, output_dict=True)\n    print(f\"Fused Accuracy: {acc}\")\n    print(f\"Fused F1 Score: {f1}\")\n    print(\"Fused Classification Report:\")\n    print(classification_report(y_true, all_preds_fused, target_names=class_names))\n\n    cm = confusion_matrix(y_true, all_preds_fused)\n    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Fused Confusion Matrix (Exponential Weighted Fusion)\")\n    plt.show()\n\n    return acc, f1, cm, report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:59:46.731292Z","iopub.execute_input":"2025-05-15T06:59:46.731484Z","iopub.status.idle":"2025-05-15T06:59:46.742003Z","shell.execute_reply.started":"2025-05-15T06:59:46.731469Z","shell.execute_reply":"2025-05-15T06:59:46.741364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion_matrices(cm_list, titles, class_names):\n    if len(cm_list) != len(titles):\n        raise ValueError(\"The number of confusion matrices must match the number of titles.\")\n\n    n = len(cm_list)\n    cols = min(n, 3)\n    rows = math.ceil(n / cols)\n\n    fig, axes = plt.subplots(rows, cols, figsize=(6 * cols, 5 * rows))\n    axes = axes.flatten() if n > 1 else [axes]\n\n    for i, (cm, title) in enumerate(zip(cm_list, titles)):\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                    xticklabels=class_names, yticklabels=class_names, ax=axes[i])\n        axes[i].set_xlabel(\"Predicted\")\n        axes[i].set_ylabel(\"True\")\n        axes[i].set_title(title)\n\n    # Hide unused subplots\n    for j in range(i + 1, len(axes)):\n        fig.delaxes(axes[j])\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:59:46.742832Z","iopub.execute_input":"2025-05-15T06:59:46.743581Z","iopub.status.idle":"2025-05-15T06:59:46.756628Z","shell.execute_reply.started":"2025-05-15T06:59:46.743563Z","shell.execute_reply":"2025-05-15T06:59:46.755949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Evaluating the Time-domain (Fused) models on Validation Set\")\ntime_acc_f, time_f1_f, time_cm_f, time_report_f = evaluate_fused_models(time_model, time_report, time_model_g, time_report_g, time_val_loader, y_val_oh, ohe.categories_[0])\nplot_confusion_matrices([time_cm, time_cm_g, time_cm_f], [f\"Original ({100*time_acc:.2f}%)\", f\"Gender ({100*time_acc_g:.2f}%)\", f\"Fused ({100*time_acc_f:.2f}%)\"], ohe.categories_[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:59:46.757309Z","iopub.execute_input":"2025-05-15T06:59:46.757539Z","iopub.status.idle":"2025-05-15T06:59:47.876650Z","shell.execute_reply.started":"2025-05-15T06:59:46.757518Z","shell.execute_reply":"2025-05-15T06:59:47.875899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Evaluating the Time-domain (Fused) models on Test Set\")\ntest_time_acc_f, test_time_f1_f, test_time_cm_f, test_time_report_g = evaluate_fused_models(time_model, time_report, time_model_g, time_report_g, time_test_loader, y_test_oh, ohe.categories_[0])\nplot_confusion_matrices([test_time_cm, test_time_cm_g, test_time_cm_f], [f\"Original ({100*test_time_acc:.2f}%)\", f\"Gender ({100*test_time_acc_g:.2f}%)\", f\"Fused ({100*test_time_acc_f:.2f}%)\"], ohe.categories_[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:59:47.877488Z","iopub.execute_input":"2025-05-15T06:59:47.877767Z","iopub.status.idle":"2025-05-15T06:59:49.077749Z","shell.execute_reply.started":"2025-05-15T06:59:47.877749Z","shell.execute_reply":"2025-05-15T06:59:49.076894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# class FreqNet(nn.Module):\n#     def __init__(self, num_classes):\n#         super(FreqNet, self).__init__()\n\n#         def conv_block(in_channels, out_channels, kernel_size):\n#             return nn.Sequential(\n#                 nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2),\n#                 nn.BatchNorm2d(out_channels),\n#                 nn.ReLU(),\n#                 nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n#                 nn.BatchNorm2d(out_channels),\n#                 nn.ReLU(),\n#                 nn.MaxPool2d(2),\n#                 nn.Dropout(0.2)\n#             )\n\n#         self.branch1 = conv_block(1, 32, kernel_size=3)\n#         self.branch2 = conv_block(1, 32, kernel_size=5)\n#         self.branch3 = conv_block(1, 32, kernel_size=7)\n\n#         self.shared_block = nn.Sequential(\n#             nn.Conv2d(96, 128, kernel_size=3, padding=1),\n#             nn.BatchNorm2d(128),\n#             nn.ReLU(),\n#             nn.Conv2d(128, 128, kernel_size=3, padding=1),\n#             nn.BatchNorm2d(128),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2),\n#             nn.Dropout(0.3)\n#         )\n\n#         self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n\n#         self.classifier = nn.Sequential(\n#             nn.Linear(128, 128),\n#             nn.ReLU(),\n#             nn.Dropout(0.4),\n#             nn.Linear(128, 64),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),\n#             nn.Linear(64, num_classes)\n#         )\n\n#     def forward(self, x):\n#         x1 = self.branch1(x)\n#         x2 = self.branch2(x)\n#         x3 = self.branch3(x)\n\n#         x_concat = torch.cat([x1, x2, x3], dim=1)\n#         x = self.shared_block(x_concat)\n#         x = self.global_pool(x)\n#         x = x.view(x.size(0), -1)\n#         return self.classifier(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:59:49.078579Z","iopub.execute_input":"2025-05-15T06:59:49.078855Z","iopub.status.idle":"2025-05-15T06:59:49.083107Z","shell.execute_reply.started":"2025-05-15T06:59:49.078829Z","shell.execute_reply":"2025-05-15T06:59:49.082322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass FreqNet(nn.Module):\n    def __init__(self, num_classes=6):  \n        super(FreqNet, self).__init__()  \n        self.conv1 = nn.Conv2d(1, 128, kernel_size=7, padding=3)\n        self.pool1 = nn.MaxPool2d(2, stride=2)\n\n        self.conv2 = nn.Conv2d(128, 256, kernel_size=7, padding=3)\n        self.pool2 = nn.MaxPool2d(2, stride=2)\n\n        self.conv3 = nn.Conv2d(256, 256, kernel_size=7, padding=3)\n        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n\n        self.fc1 = nn.Linear(256, 64)\n        self.fc2 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))   # (B, 128, H, W)\n        x = self.pool1(x)           # (B, 128, H/2, W/2)\n        x = F.relu(self.conv2(x))   # (B, 256, H/2, W/2)\n        x = self.pool2(x)           # (B, 256, H/4, W/4)\n        x = F.relu(self.conv3(x))   # (B, 256, H/4, W/4)\n        x = self.global_avg_pool(x) # (B, 256, 1, 1)\n        x = x.view(x.size(0), -1)   # (B, 256)\n        x = F.relu(self.fc1(x))     # (B, 64)\n        x = self.fc2(x)             # (B, num_classes)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T07:01:08.947949Z","iopub.execute_input":"2025-05-15T07:01:08.948703Z","iopub.status.idle":"2025-05-15T07:01:08.955251Z","shell.execute_reply.started":"2025-05-15T07:01:08.948673Z","shell.execute_reply":"2025-05-15T07:01:08.954452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"set_seed(RANDOM_SEED)\nmel_model = FreqNet(num_classes=y_train_oh.shape[1])\n\nmel_model, mel_acc, mel_f1, mel_cm, mel_report = train_model(\n    mel_model,\n    mel_train_loader,\n    mel_val_loader,\n    y_val_oh,\n    class_names=ohe.categories_[0],\n    lr=0.00001\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"set_seed(RANDOM_SEED)\nmel_model_g = FreqNet(num_classes=y_train_oh_g.shape[1])\n\nmel_model_g, mel_acc_g, mel_f1_g, mel_cm_g, mel_report_g = train_model(\n    mel_model_g,\n    mel_train_loader_g,\n    mel_val_loader_g,\n    y_val_oh_g,\n    class_names=ohe_g.categories_[0],\n    pool = True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Evaluating the Frequency-domain model on Test Set\")\ntest_mel_acc, test_mel_f1, test_mel_cm, test_mel_report = evaluate_model(\n    mel_model,\n    mel_test_loader,\n    y_test_oh,\n    class_names=ohe.categories_[0]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T07:51:01.850225Z","iopub.execute_input":"2025-05-15T07:51:01.850514Z","iopub.status.idle":"2025-05-15T07:51:05.181580Z","shell.execute_reply.started":"2025-05-15T07:51:01.850492Z","shell.execute_reply":"2025-05-15T07:51:05.180884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Evaluating the Frequency-domain (Gender) model on Test Set\")\ntest_mel_acc_g, test_mel_f1_g, test_mel_cm_g, test_mel_report_g = evaluate_model(\n    mel_model_g,\n    mel_test_loader_g,\n    y_test_oh_g,\n    class_names=ohe_g.categories_[0],\n    pool = True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:59:49.204046Z","iopub.status.idle":"2025-05-15T06:59:49.204352Z","shell.execute_reply.started":"2025-05-15T06:59:49.204169Z","shell.execute_reply":"2025-05-15T06:59:49.204185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Evaluating the Fequency-domain (Fused) models on Validation Set\")\nmel_acc_f, mel_f1_f, mel_cm_f, mel_report_f = evaluate_fused_models(mel_model, mel_report, mel_model_g, mel_report_g, mel_val_loader, y_val_oh, ohe.categories_[0])\nplot_confusion_matrices([mel_cm, mel_cm_g, mel_cm_f], [f\"Original ({100*mel_acc:.2f}%)\", f\"Gender ({100*mel_acc_g:.2f}%)\", f\"Fused ({100*mel_acc_f:.2f}%)\"], ohe.categories_[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:59:49.206098Z","iopub.status.idle":"2025-05-15T06:59:49.206357Z","shell.execute_reply.started":"2025-05-15T06:59:49.206238Z","shell.execute_reply":"2025-05-15T06:59:49.206249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Evaluating the Frequency-domain (Fused) models on Test Set\")\ntest_mel_acc_f, test_mel_f1_f, test_mel_cm_f, test_mel_report_f = evaluate_fused_models(mel_model, mel_report, mel_model_g, mel_report_g, mel_test_loader, y_test_oh, ohe.categories_[0])\nplot_confusion_matrices([test_mel_cm, test_mel_cm_g, test_mel_cm_f], [f\"Original ({100*test_mel_acc:.2f}%)\", f\"Gender ({100*test_mel_acc_g:.2f}%)\", f\"Fused ({100*test_mel_acc_f:.2f}%)\"], ohe.categories_[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:59:49.207221Z","iopub.status.idle":"2025-05-15T06:59:49.207424Z","shell.execute_reply.started":"2025-05-15T06:59:49.207329Z","shell.execute_reply":"2025-05-15T06:59:49.207338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ALL Models with best parameters","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Paths to model files\nmodel_paths = {\n    \"mel_model\": \"/kaggle/input/mel_model/pytorch/default/1/mel_model.pth\",\n    \"mel_model_g\": \"/kaggle/input/mel_model_g/pytorch/default/1/mel_model_g.pth\",\n    \"time_model\": \"/kaggle/input/time_model/pytorch/default/1/time_model.pth\",\n    \"time_model_g\": \"/kaggle/input/time_model_g/pytorch/default/1/time_model_g.pth\"\n}\n\n# Class mapping for each model\nmodel_classes = {\n    \"mel_model\": FreqNet,\n    \"mel_model_g\": FreqNet,\n    \"time_model\": TimeNet,\n    \"time_model_g\": TimeNet\n}\n\n# Correct number of output classes per model\nmodel_num_classes = {\n    \"mel_model\": 6,\n    \"mel_model_g\": 12,\n    \"time_model\": 6,\n    \"time_model_g\": 12\n}\n\n# Dictionary to hold loaded models\nloaded_models = {}\n\n# Load each model properly\nfor name, path in model_paths.items():\n    model_class = model_classes[name]\n    num_classes = model_num_classes[name]\n    \n    # Instantiate and load the model\n    model = model_class(num_classes=num_classes)\n    model.load_state_dict(torch.load(path, map_location=device, weights_only=True))\n    model.to(device)\n    model.eval()\n    \n    loaded_models[name] = model\n    print(f\"Loaded and ready: {name}\")\n\nmel_model = loaded_models[\"mel_model\"]\nmel_model_g = loaded_models[\"mel_model_g\"]\ntime_model_g = loaded_models[\"time_model_g\"]\ntime_model = loaded_models[\"time_model\"]\n# mel_model.eval()\n# mel_model_g.eval()\n# time_model_g.eval()\n# time_model.eval()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:29:49.858992Z","iopub.execute_input":"2025-05-15T11:29:49.859725Z","iopub.status.idle":"2025-05-15T11:29:49.988540Z","shell.execute_reply.started":"2025-05-15T11:29:49.859698Z","shell.execute_reply":"2025-05-15T11:29:49.987765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mel_acc, mel_f1, mel_cm, mel_report = evaluate_model(\n    mel_model,\n    mel_val_loader,\n    y_val_oh,\n    class_names=ohe.categories_[0]\n)\n\nprint(\"Evaluating the Frequency-domain model on Test Set\")\ntest_mel_acc, test_mel_f1, test_mel_cm, test_mel_report = evaluate_model(\n    mel_model,\n    mel_test_loader,\n    y_test_oh,\n    class_names=ohe.categories_[0]\n)\nmel_acc_g, mel_f1_g, mel_cm_g, mel_report_g = evaluate_model(\n    mel_model_g,\n    mel_val_loader_g,\n    y_val_oh_g,\n    class_names=ohe_g.categories_[0],\n    pool = True\n)\n\nprint(\"Evaluating the Frequency-domain (Gender) model on Test Set\")\ntest_mel_acc_g, test_mel_f1_g, test_mel_cm_g, test_mel_report_g = evaluate_model(\n    mel_model_g,\n    mel_test_loader_g,\n    y_test_oh_g,\n    class_names=ohe_g.categories_[0],\n    pool = True\n)\nprint(\"Evaluating the Frequency-domain (Fused) models on Test Set\")\ntest_mel_acc_f, test_mel_f1_f, test_mel_cm_f, test_mel_report_f = evaluate_fused_models(mel_model, mel_report, mel_model_g, mel_report_g, mel_test_loader, y_test_oh, ohe.categories_[0])\nplot_confusion_matrices([test_mel_cm, test_mel_cm_g, test_mel_cm_f], [f\"Original ({100*test_mel_acc:.2f}%)\", f\"Gender ({100*test_mel_acc_g:.2f}%)\", f\"Fused ({100*test_mel_acc_f:.2f}%)\"], ohe.categories_[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:33:26.079309Z","iopub.execute_input":"2025-05-15T11:33:26.080006Z","iopub.status.idle":"2025-05-15T11:33:40.916546Z","shell.execute_reply.started":"2025-05-15T11:33:26.079975Z","shell.execute_reply":"2025-05-15T11:33:40.915756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ohe.categories_[0])\ntime_acc, time_f1, time_cm, time_report = evaluate_model(time_model, time_val_loader, y_val_oh, class_names=ohe.categories_[0])\n\nprint(\"Evaluating the Time-domain model on Test Set\")\ntest_time_acc, test_time_f1, test_time_cm, test_time_report = evaluate_model(time_model, time_test_loader, y_test_oh, class_names=ohe.categories_[0])\n\ntime_acc_g, time_f1_g, time_cm_g, time_report_g = evaluate_model(time_model_g, time_val_loader_g, y_val_oh_g, class_names=ohe_g.categories_[0], pool = True)\n\nprint(\"Evaluating the Time-domain (Gender) model on Test Set\")\ntest_time_acc_g, test_time_f1_g, test_time_cm_g, test_time_report_g = evaluate_model(time_model_g, time_test_loader_g, y_test_oh_g, class_names=ohe_g.categories_[0], pool = True)\n\nprint(\"Evaluating the Time-domain (Fused) models on Validation Set\")\ntime_acc_f, time_f1_f, time_cm_f, time_report_f = evaluate_fused_models(time_model, time_report, time_model_g, time_report_g, time_val_loader, y_val_oh, ohe.categories_[0])\nplot_confusion_matrices([time_cm, time_cm_g, time_cm_f], [f\"Original ({100*time_acc:.2f}%)\", f\"Gender ({100*time_acc_g:.2f}%)\", f\"Fused ({100*time_acc_f:.2f}%)\"], ohe.categories_[0])\nprint(\"Evaluating the Time-domain (Fused) models on Test Set\")\n\nprint(\"Evaluating the Time-domain (Fused) models on Test Set\")\ntest_time_acc_f, test_time_f1_f, test_time_cm_f, test_time_report_g = evaluate_fused_models(time_model, time_report, time_model_g, time_report_g, time_test_loader, y_test_oh, ohe.categories_[0])\nplot_confusion_matrices([test_time_cm, test_time_cm_g, test_time_cm_f], [f\"Original ({100*test_time_acc:.2f}%)\", f\"Gender ({100*test_time_acc_g:.2f}%)\", f\"Fused ({100*test_time_acc_f:.2f}%)\"], ohe.categories_[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:32:49.114329Z","iopub.execute_input":"2025-05-15T11:32:49.114895Z","iopub.status.idle":"2025-05-15T11:32:52.539817Z","shell.execute_reply.started":"2025-05-15T11:32:49.114871Z","shell.execute_reply":"2025-05-15T11:32:52.539066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_confusion_matrices([\n    test_time_cm, test_time_cm_g, test_time_cm_f, \n    test_mel_cm, test_mel_cm_g, test_mel_cm_f], [\n    f\"Original Time ({100*test_time_acc:.2f}%)\", f\"Gender Time ({100*test_time_acc_g:.2f}%)\", f\"Fused Time ({100*test_time_acc_f:.2f}%)\", \n    f\"Original Mel ({100*test_mel_acc:.2f}%)\", f\"Gender Mel ({100*test_mel_acc_g:.2f}%)\", f\"Fused Mel ({100*test_mel_acc_f:.2f}%)\",], \n    ohe.categories_[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:33:40.917533Z","iopub.execute_input":"2025-05-15T11:33:40.917754Z","iopub.status.idle":"2025-05-15T11:33:43.139937Z","shell.execute_reply.started":"2025-05-15T11:33:40.917736Z","shell.execute_reply":"2025-05-15T11:33:43.139015Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# hyperparameter Tuning ","metadata":{}},{"cell_type":"code","source":"\ndef tune_and_save_best_model(\n    model_class,\n    param_grid,\n    train_loader,\n    val_loader,\n    y_val,\n    class_names,\n    num_classes,\n    model_name, \n    output_dir='/kaggle/working'\n):\n    best_f1 = -1\n    best_model = None\n    best_params = {}\n    best_acc = 0.0\n\n    all_combinations = list(itertools.product(*param_grid.values()))\n\n    for combo in all_combinations:\n        params = dict(zip(param_grid.keys(), combo))\n        print(f\"\\nTrying config: {params}...\")\n\n        model = model_class(num_classes=num_classes)\n\n        trained_model, acc, f1, cm, report = train_model(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            y_val=y_val,\n            class_names=class_names,\n            epochs=params.get('epochs', 100),\n            lr=params.get('lr', 0.001),\n            patience=params.get('patience', 10),\n            isTuning = False\n        )\n\n        print(f\"F1 Score = {f1:.4f}, Accuracy = {acc:.4f}\")\n\n        if f1 > best_f1:\n            best_f1 = f1\n            best_acc = acc\n            best_model = trained_model\n            best_params = params\n\n    if best_model:\n        # Save under /kaggle/working/models/model_name/\n        model_dir = os.path.join(output_dir, \"models\", model_name)\n        os.makedirs(model_dir, exist_ok=True)\n\n        # base_filename = f\"{model_name}_f1_{best_f1:.4f}_acc_{best_acc:.4f}\"\n        # save_path = os.path.join(model_dir, base_filename + \".pth\")\n        # meta_path = os.path.join(model_dir, base_filename + \"_meta.json\")\n        save_path = os.path.join(model_dir, f\"{model_name}.pth\")\n        meta_path = os.path.join(model_dir, f\"{model_name}_meta.json\")\n        # Save model weights\n        torch.save(best_model.state_dict(), save_path)\n\n        # Save metadata\n        meta = {\n            \"best_params\": best_params,\n            \"best_f1\": best_f1,\n            \"best_acc\": best_acc,\n            \"model_path\": save_path\n        }\n        with open(meta_path, \"w\") as f:\n            json.dump(meta, f, indent=4)\n\n        print(f\"\\n Best model saved to: {save_path}\")\n        print(f\"Metadata saved to: {meta_path}\")\n    else:\n        save_path = None\n        print(\" No valid model was trained.\")\n    best_model.eval()\n    return best_params, best_f1, save_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T07:55:17.500039Z","iopub.execute_input":"2025-05-15T07:55:17.500320Z","iopub.status.idle":"2025-05-15T07:55:17.508916Z","shell.execute_reply.started":"2025-05-15T07:55:17.500297Z","shell.execute_reply":"2025-05-15T07:55:17.508230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_grid = {\n    'lr': [0.005, 0.001, 0.0001],\n    'epochs': [50, 100],\n    'patience': [10, 15]\n}\n\n\nbest_params, best_f1, saved_path = tune_and_save_best_model(\n    model_class=TimeNet,\n    param_grid=param_grid,\n    train_loader=time_train_loader,\n    val_loader=time_val_loader,\n    y_val=y_val_oh,\n    class_names=ohe.categories_[0],\n    num_classes=y_train_oh.shape[1],\n    model_name=\"time_Model\"\n)\n# best_params, best_f1, saved_path = tune_and_save_best_model(\n#     model_class=FreqNet,  \n#     param_grid=param_grid,\n#     train_loader=mel_train_loader,\n#     val_loader=mel_val_loader,\n#     y_val=y_val_oh,\n#     class_names=ohe.categories_[0],\n#     num_classes=y_train_oh.shape[1],  \n#     model_name=\"mal1_Model\"\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T07:55:20.107298Z","iopub.execute_input":"2025-05-15T07:55:20.107574Z","iopub.status.idle":"2025-05-15T07:59:35.268021Z","shell.execute_reply.started":"2025-05-15T07:55:20.107553Z","shell.execute_reply":"2025-05-15T07:59:35.267356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_grid = {\n    'lr': [0.005, 0.001],\n    'epochs': [20, 30],\n    'patience': [5, 10]\n}\n\nbest_params, best_f1, saved_path = tune_and_save_best_model(\n    model_class=FreqNet,  \n    param_grid=param_grid,\n    train_loader=mel_train_loader,\n    val_loader=mel_val_loader,\n    y_val=y_val_oh,\n    class_names=ohe.categories_[0],\n    num_classes=y_train_oh.shape[1],  \n    model_name=\"mal_Model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T08:01:00.431443Z","iopub.execute_input":"2025-05-15T08:01:00.432050Z","iopub.status.idle":"2025-05-15T09:50:58.267910Z","shell.execute_reply.started":"2025-05-15T08:01:00.432028Z","shell.execute_reply":"2025-05-15T09:50:58.267253Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef load_model_from_folder(model_class, model_name, num_classes, base_path='/kaggle/working/models'):\n    model_dir = os.path.join(base_path, model_name)\n    \n    if not os.path.exists(model_dir):\n        raise FileNotFoundError(f\"Directory '{model_dir}' does not exist.\")\n    \n    # Load model weights\n    pth_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]\n    if not pth_files:\n        raise FileNotFoundError(f\"No .pth file found in '{model_dir}'.\")\n    model_path = os.path.join(model_dir, pth_files[0])\n    \n    model = model_class(num_classes=num_classes)\n    state_dict = torch.load(model_path, weights_only=True)\n    model.load_state_dict(state_dict)\n    model.eval()\n    \n    print(f\"Loaded model from: {model_path}\")\n    \n    # Load best parameters if present\n    best_params_path = os.path.join(model_dir, f\"{model_name}_meta.json\")\n    best_params = None\n    if os.path.exists(best_params_path):\n        with open(best_params_path, 'r') as f:\n            best_params = json.load(f)\n        # print(f\"Loaded best parameters: {best_params}\")\n    else:\n        print(\"No best_params.json found.\")\n\n    return model, best_params\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:52:32.908213Z","iopub.execute_input":"2025-05-15T09:52:32.908931Z","iopub.status.idle":"2025-05-15T09:52:32.915953Z","shell.execute_reply.started":"2025-05-15T09:52:32.908905Z","shell.execute_reply":"2025-05-15T09:52:32.915123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Evaluating the Time-domain (Gender) model on Test Set\")\n\n# Load the model\ntime_model2,best_params = load_model_from_folder(TimeNet, \"time_Model\", num_classes=y_train_oh.shape[1])\nprint(best_params)\ntime_model2.eval()  \n\n# Evaluate\ntest_time_acc_g, test_time_f1_g, test_time_cm_g, test_time_report_g = evaluate_model(time_model2, time_test_loader, y_test_oh, class_names=ohe.categories_[0])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T09:52:34.899575Z","iopub.execute_input":"2025-05-15T09:52:34.899871Z","iopub.status.idle":"2025-05-15T09:52:35.508186Z","shell.execute_reply.started":"2025-05-15T09:52:34.899850Z","shell.execute_reply":"2025-05-15T09:52:35.507379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Evaluating the Mel-spectrogram (Emotion) model on Test Set\")\n\n# Load the model\nmel_model,best_params = load_model_from_folder(FreqNet, \"mal_Model\", num_classes=y_train_oh.shape[1])\nprint(best_params)\n# Evaluate\ntest_mel_acc, test_mel_f1, test_mel_cm, test_mel_report = evaluate_model(\n    mel_model,\n    mel_test_loader,           \n    y_test_oh,                 \n    class_names=ohe.categories_[0],\n    # pool=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:14:48.904413Z","iopub.execute_input":"2025-05-15T10:14:48.904721Z","iopub.status.idle":"2025-05-15T10:19:44.742836Z","shell.execute_reply.started":"2025-05-15T10:14:48.904699Z","shell.execute_reply":"2025-05-15T10:19:44.742268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil\n# import os\n\n# model_dir = \"/kaggle/working/models/time_Model\"\n\n# if os.path.exists(model_dir):\n#     shutil.rmtree(model_dir)\n#     print(f\"Deleted folder: {model_dir}\")\n# else:\n#     print(f\"Folder not found: {model_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:59:49.216458Z","iopub.status.idle":"2025-05-15T06:59:49.216799Z","shell.execute_reply.started":"2025-05-15T06:59:49.216588Z","shell.execute_reply":"2025-05-15T06:59:49.216620Z"}},"outputs":[],"execution_count":null}]}